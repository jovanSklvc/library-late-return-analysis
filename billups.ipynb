{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9780e4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with 2000 raw records.\n",
      "Pre-cleaned date columns to remove special characters and incorrect formats.\n",
      "Removed 84 records with future checkout dates.\n",
      "Removed 153 records with future return dates.\n",
      "Removed 90 records with checkout dates before 2005.\n",
      "Removed 157 records where return date was before checkout date.\n",
      "\n",
      "Ending with 1451 valid records to be used for analysis.\n",
      "--------------------------------------------------\n",
      "\n",
      "Successfully processed the data with the corrected rules!\n",
      "\n",
      "The late return rate on this fully validated data is: 9.10%\n",
      "\n",
      "Preview of the final, clean DataFrame:\n",
      "Cleaned data has been saved to 'cleaned_checkouts.csv'\n",
      "             id                         patron_id           library_id  \\\n",
      "1  HUX-y4oXl04C  8d3f63e1deed89d7ba1bf6a4eb101373  223-222@5xc-jxr-tgk   \n",
      "2  TQpFnkku2poC  4ae202f8de762591734705e0079d76df  228-222@5xc-jtz-hwk   \n",
      "3  OQ6sDwAAQBAJ  f9372de3c8ea501601aa3fb59ec0f524  23v-222@5xc-jv7-v4v   \n",
      "6  CW-7tHAaVR0C  dd9f34e9d65126a2b02003d8ac60aaa4  22c-222@5xc-jwj-pvz   \n",
      "8  t1e3BWziAc8C  3b85b2c7b424618f533329018e9a11d5  222-222@5xc-jv5-nt9   \n",
      "\n",
      "  date_checkout date_returned  loan_duration_days   status  \n",
      "1    2018-05-29    2018-06-12                  14  On Time  \n",
      "2    2018-11-23    2019-01-24                  62     Late  \n",
      "3    2018-01-15    2018-04-25                 100     Late  \n",
      "6    2018-01-10    2018-02-04                  25  On Time  \n",
      "8    2018-06-23    2018-07-14                  21  On Time  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jovan\\AppData\\Local\\Temp\\ipykernel_2652\\245197525.py:57: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  checkouts_df['date_returned'].fillna(current_date, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Define a Custom Date Cleaning Function ---\n",
    "def clean_date_format(date_val):\n",
    "    \"\"\"\n",
    "    Strips special characters and non-digit characters from a date string,\n",
    "    preparing it for conversion.\n",
    "    \"\"\"\n",
    "    if pd.isna(date_val):\n",
    "        return np.nan\n",
    "    # Convert to string and remove all non-digit characters\n",
    "    cleaned_str = ''.join(filter(str.isdigit, str(date_val)))\n",
    "    # Ensure it's a plausible length for a date (YYYYMMDD)\n",
    "    return cleaned_str if len(cleaned_str) == 8 else np.nan\n",
    "\n",
    "# --- 2. Load and Pre-clean the Dataset ---\n",
    "checkouts_df = pd.read_csv('data\\\\checkouts.csv')\n",
    "print(f\"Starting with {len(checkouts_df)} raw records.\")\n",
    "\n",
    "# Apply the custom cleaning function first\n",
    "checkouts_df['date_checkout'] = checkouts_df['date_checkout'].apply(clean_date_format)\n",
    "checkouts_df['date_returned'] = checkouts_df['date_returned'].apply(clean_date_format)\n",
    "print(\"Pre-cleaned date columns to remove special characters and incorrect formats.\")\n",
    "\n",
    "# --- 3. Master Date Conversion and Validation ---\n",
    "# Now, convert the clean strings to datetime objects\n",
    "checkouts_df['date_checkout'] = pd.to_datetime(checkouts_df['date_checkout'], errors='coerce')\n",
    "checkouts_df['date_returned'] = pd.to_datetime(checkouts_df['date_returned'], errors='coerce')\n",
    "\n",
    "# Drop any rows where the checkout date is invalid after cleaning\n",
    "checkouts_df.dropna(subset=['date_checkout'], inplace=True)\n",
    "\n",
    "# Define the boundaries for our analysis\n",
    "current_date = pd.Timestamp.now()\n",
    "earliest_valid_date = pd.Timestamp('2005-01-01')\n",
    "\n",
    "# Apply the date validation filters\n",
    "initial_rows = len(checkouts_df)\n",
    "\n",
    "# RULE 1: Remove future checkout dates\n",
    "checkouts_df = checkouts_df[checkouts_df['date_checkout'] <= current_date]\n",
    "print(f\"Removed {initial_rows - len(checkouts_df)} records with future checkout dates.\")\n",
    "initial_rows = len(checkouts_df)\n",
    "\n",
    "# RULE 2: Remove future return dates\n",
    "checkouts_df = checkouts_df[checkouts_df['date_returned'] <= current_date]\n",
    "print(f\"Removed {initial_rows - len(checkouts_df)} records with future return dates.\")\n",
    "initial_rows = len(checkouts_df)\n",
    "\n",
    "# RULE 3: Remove checkouts before the start date of 2005\n",
    "checkouts_df = checkouts_df[checkouts_df['date_checkout'] >= earliest_valid_date]\n",
    "print(f\"Removed {initial_rows - len(checkouts_df)} records with checkout dates before {earliest_valid_date.year}.\")\n",
    "initial_rows = len(checkouts_df)\n",
    "\n",
    "# Fill missing return dates with the current date\n",
    "checkouts_df['date_returned'].fillna(current_date, inplace=True)\n",
    "\n",
    "# RULE 4: Remove records where the return happens before the checkout\n",
    "checkouts_df = checkouts_df[checkouts_df['date_returned'] >= checkouts_df['date_checkout']]\n",
    "print(f\"Removed {initial_rows - len(checkouts_df)} records where return date was before checkout date.\")\n",
    "\n",
    "print(f\"\\nEnding with {len(checkouts_df)} valid records to be used for analysis.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# --- 4. Feature Engineering ---\n",
    "checkouts_df['loan_duration_days'] = (checkouts_df['date_returned'] - checkouts_df['date_checkout']).dt.days\n",
    "checkouts_df['status'] = np.where(checkouts_df['loan_duration_days'] > 28, 'Late', 'On Time')\n",
    "\n",
    "# --- 5. Final Output ---\n",
    "print(\"\\nSuccessfully processed the data with the corrected rules!\")\n",
    "\n",
    "late_rate = checkouts_df['status'].value_counts(normalize=True).get('Late', 0) * 100\n",
    "print(f\"\\nThe late return rate on this fully validated data is: {late_rate:.2f}%\")\n",
    "\n",
    "print(\"\\nPreview of the final, clean DataFrame:\")\n",
    "cleaned_checkouts_file = 'cleaned_checkouts.csv'\n",
    "checkouts_df.to_csv(cleaned_checkouts_file, index=False)\n",
    "print(f\"Cleaned data has been saved to '{cleaned_checkouts_file}'\")\n",
    "print(checkouts_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f073fb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting with 2000 records, which will all be preserved. ---\n",
      "Cleaned text columns.\n",
      "Created temporary validated columns for quality check.\n",
      "Created 'data_quality' flag column based on all validation rules.\n",
      "\n",
      "--- Data Quality Report ---\n",
      "data_quality\n",
      "OK                                1707\n",
      "Missing Birth Date                 107\n",
      "Future Birth Date                   93\n",
      "Implausible Age                     92\n",
      "Invalid ZIP, Future Birth Date       1\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Final Data Preview ---\n",
      "                name        street_address zipcode  birth_date  \\\n",
      "0  Cynthia Barnfield       44 NE Meikle PL   97213  2009-09-10   \n",
      "1    Elizabeth Smith   7511 SE Harrison ST   97215  1956-12-15   \n",
      "2      Richard Pabla       1404 SE Pine ST   97214  1960-12-18   \n",
      "3      Charles Baker  12271 N Westshore DR   97217  2105-07-19   \n",
      "4       Ronald Lydon   5321 NE Skyport Way   97218  1961-03-14   \n",
      "\n",
      "        data_quality  \n",
      "0                 OK  \n",
      "1                 OK  \n",
      "2                 OK  \n",
      "3  Future Birth Date  \n",
      "4                 OK  \n",
      "\n",
      "Saved the final, clean and enriched customer data to 'cleaned_customers.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Load the customers dataset ---\n",
    "customers_df = pd.read_csv('data\\\\customers.csv')\n",
    "print(f\"--- Starting with {len(customers_df)} records, which will all be preserved. ---\")\n",
    "\n",
    "# --- 2. Define Custom Cleaning Functions ---\n",
    "def clean_street_address(address):\n",
    "    \"\"\"Applies custom capitalization rules to a street address.\"\"\"\n",
    "    if pd.isna(address):\n",
    "        return np.nan\n",
    "    address = ' '.join(str(address).strip().split())\n",
    "    processed_words = [word.upper() if len(word) <= 2 else word.capitalize() for word in address.split(' ')]\n",
    "    return ' '.join(processed_words)\n",
    "\n",
    "def validate_zipcode(zip_val):\n",
    "    \"\"\"Validates ZIP code is 5 digits, else returns 'Unknown'.\"\"\"\n",
    "    if pd.isna(zip_val):\n",
    "        return \"Unknown\"\n",
    "    cleaned_zip = ''.join(filter(str.isdigit, str(zip_val).split('.')[0]))\n",
    "    return cleaned_zip if len(cleaned_zip) == 5 else \"Unknown\"\n",
    "\n",
    "# --- 3. Apply Cleaning and Create Validated Columns ---\n",
    "customers_df['name'] = customers_df['name'].astype(str).str.replace(r'[^a-zA-Z\\s]', '', regex=True).str.replace(r'\\s+', ' ', regex=True).str.strip().str.title()\n",
    "customers_df['street_address'] = customers_df['street_address'].apply(clean_street_address)\n",
    "\n",
    "# UPDATED LOOP: Now includes 'education' and 'occupation'\n",
    "for col in ['city', 'state', 'education', 'occupation']:\n",
    "    customers_df[col] = customers_df[col].astype(str).str.replace(r'\\s+', ' ', regex=True).str.strip().str.title().replace('Nan', np.nan)\n",
    "print(\"Cleaned text columns.\")\n",
    "\n",
    "customers_df['zipcode_validated'] = customers_df['zipcode'].apply(validate_zipcode)\n",
    "customers_df['birth_date_cleaned'] = pd.to_datetime(customers_df['birth_date'], errors='coerce')\n",
    "print(\"Created temporary validated columns for quality check.\")\n",
    "\n",
    "# --- 4. Create the Data Quality Flag Column ---\n",
    "issues = []\n",
    "now = pd.Timestamp.now()\n",
    "earliest_valid_date = pd.Timestamp('1930-01-01')\n",
    "\n",
    "for index, row in customers_df.iterrows():\n",
    "    row_issues = []\n",
    "    if row['zipcode_validated'] == 'Unknown':\n",
    "        row_issues.append('Invalid ZIP')\n",
    "\n",
    "    birth_date = row['birth_date_cleaned']\n",
    "    if pd.isna(birth_date):\n",
    "        row_issues.append('Missing Birth Date')\n",
    "    else:\n",
    "        if birth_date < earliest_valid_date:\n",
    "            row_issues.append('Implausible Age')\n",
    "        if birth_date > now:\n",
    "            row_issues.append('Future Birth Date')\n",
    "\n",
    "    if not row_issues:\n",
    "        issues.append('OK')\n",
    "    else:\n",
    "        issues.append(', '.join(row_issues))\n",
    "\n",
    "customers_df['data_quality'] = issues\n",
    "print(\"Created 'data_quality' flag column based on all validation rules.\")\n",
    "\n",
    "# --- 5. Finalize the DataFrame ---\n",
    "customers_df['zipcode'] = customers_df['zipcode_validated']\n",
    "customers_df = customers_df.drop(columns=['zipcode_validated', 'birth_date_cleaned'])\n",
    "\n",
    "# --- 6. Final Output ---\n",
    "print(\"\\n--- Data Quality Report ---\")\n",
    "print(customers_df['data_quality'].value_counts())\n",
    "print(\"-\" * 50)\n",
    "print(\"\\n--- Final Data Preview ---\")\n",
    "print(customers_df[['name', 'street_address', 'zipcode', 'birth_date', 'data_quality']].head())\n",
    "\n",
    "# Save the cleaned and enriched data\n",
    "customers_df.to_csv('cleaned_customers.csv', index=False)\n",
    "print(\"\\nSaved the final, clean and enriched customer data to 'cleaned_customers.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edab506e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting with 240 records ---\n",
      "Cleaned 'id' column: Removed leading '=' from 0 records.\n",
      "Created 'publishedYear' column without altering 'publishedDate'.\n",
      "Cleaned 'categories': Removed brackets, single quotes, and double quotes.\n",
      "Verification PASSED: All specified special characters successfully removed from 'categories'.\n",
      "Cleaned 'price': Extracted numbers and converted to numeric.\n",
      "Cleaned 'pages': Extracted numbers and converted to Integer.\n",
      "\n",
      "--- Final Data Preview ---\n",
      "             id publishedDate  publishedYear               categories   price  \\\n",
      "0  hVFwAAAAQBAJ    2013-09-11           2013           Social Science   72.99   \n",
      "1  bRY9AAAAYAAJ          1913           1913              Advertising  469.99   \n",
      "2  ZapAAAAAIAAJ          1973           1973              Advertising  372.00   \n",
      "3  A-HthMfF5moC          1894           1894              Advertising  240.99   \n",
      "4  4Z9JAAAAMAAJ          1944           1944  Government publications  539.00   \n",
      "\n",
      "   pages  \n",
      "0    320  \n",
      "1    654  \n",
      "2    784  \n",
      "3    559  \n",
      "4    757  \n",
      "\n",
      "--- Final Data Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 240 entries, 0 to 239\n",
      "Data columns (total 9 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   id             240 non-null    object \n",
      " 1   title          240 non-null    object \n",
      " 2   authors        173 non-null    object \n",
      " 3   publisher      94 non-null     object \n",
      " 4   publishedDate  238 non-null    object \n",
      " 5   categories     201 non-null    object \n",
      " 6   price          238 non-null    float64\n",
      " 7   pages          240 non-null    Int64  \n",
      " 8   publishedYear  238 non-null    Int64  \n",
      "dtypes: Int64(2), float64(1), object(6)\n",
      "memory usage: 17.5+ KB\n",
      "\n",
      "Saved the final, clean books data to 'cleaned_books.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Load the books dataset ---\n",
    "books_df = pd.read_csv('data\\\\books.csv')\n",
    "print(f\"--- Starting with {len(books_df)} records ---\")\n",
    "\n",
    "# --- 2. NEW: Clean the 'id' Column ---\n",
    "# Remove any leading equals signs to prevent Excel formula errors\n",
    "original_ids = books_df['id'].copy()\n",
    "books_df['id'] = books_df['id'].str.lstrip('=')\n",
    "changes = (books_df['id'] != original_ids).sum()\n",
    "print(f\"Cleaned 'id' column: Removed leading '=' from {changes} records.\")\n",
    "\n",
    "# --- 3. Create 'publishedYear' Column ---\n",
    "year_str = books_df['publishedDate'].astype(str).str.slice(0, 4)\n",
    "books_df['publishedYear'] = pd.to_numeric(year_str, errors='coerce')\n",
    "books_df['publishedYear'] = books_df['publishedYear'].astype(pd.Int64Dtype())\n",
    "print(\"Created 'publishedYear' column without altering 'publishedDate'.\")\n",
    "\n",
    "# --- 4. Clean 'categories' Column ---\n",
    "books_df['categories'] = books_df['categories'].astype(str).str.replace(r\"\\[|\\]|'|\\\"\", \"\", regex=True)\n",
    "books_df['categories'] = books_df['categories'].replace('nan', np.nan)\n",
    "print(\"Cleaned 'categories': Removed brackets, single quotes, and double quotes.\")\n",
    "\n",
    "# --- 5. Verification Step ---\n",
    "remaining_issues = books_df['categories'].str.contains(r\"\\[|\\]|'|\\\"\", regex=True, na=False)\n",
    "if remaining_issues.any():\n",
    "    print(\"Verification FAILED: Special characters still found in the 'categories' column.\")\n",
    "else:\n",
    "    print(\"Verification PASSED: All specified special characters successfully removed from 'categories'.\")\n",
    "\n",
    "# --- 6. Clean 'price' Column ---\n",
    "books_df['price'] = books_df['price'].astype(str).str.extract(r'(\\d+\\.?\\d*)')\n",
    "books_df['price'] = pd.to_numeric(books_df['price'], errors='coerce')\n",
    "print(\"Cleaned 'price': Extracted numbers and converted to numeric.\")\n",
    "\n",
    "# --- 7. Clean 'pages' Column ---\n",
    "books_df['pages'] = books_df['pages'].astype(str).str.extract(r'(\\d+)')\n",
    "books_df['pages'] = pd.to_numeric(books_df['pages'], errors='coerce')\n",
    "books_df['pages'] = books_df['pages'].astype(pd.Int64Dtype())\n",
    "print(\"Cleaned 'pages': Extracted numbers and converted to Integer.\")\n",
    "\n",
    "# --- 8. Final Output ---\n",
    "print(\"\\n--- Final Data Preview ---\")\n",
    "print(books_df[['id', 'publishedDate', 'publishedYear', 'categories', 'price', 'pages']].head())\n",
    "print(\"\\n--- Final Data Info ---\")\n",
    "books_df.info()\n",
    "\n",
    "# Save the cleaned data\n",
    "books_df.to_csv('cleaned_books.csv', index=False)\n",
    "print(\"\\nSaved the final, clean books data to 'cleaned_books.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a49b6515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned 'name': Collapsed spaces and applied title case.\n",
      "Cleaned 'street_address': Applied custom capitalization.\n",
      "Cleaned 'city': Trimmed whitespace and capitalized.\n",
      "Cleaned 'region': Trimmed whitespace and converted to uppercase.\n",
      "Cleaned 'postal_code': Removed special characters. Updated 14 records.\n",
      "\n",
      "--- Final Data Preview ---\n",
      "                    id                                        name  \\\n",
      "0  226-222@5xc-kc4-fpv       Multnomah County Library Capitol Hill   \n",
      "1  23v-222@5xc-jv7-v4v          Multnomah County Library Northwest   \n",
      "2  222-222@5xc-jvf-skf           Multnomah County Library St Johns   \n",
      "3  227-222@5xc-jww-btv          Multnomah County Library Hillsdale   \n",
      "4  22d-222@5xc-kcy-8sq  Multnomah County Library Sellwood Moreland   \n",
      "\n",
      "          street_address      city region postal_code  \n",
      "0   10723 SW Capitol Hwy  Portland     OR       97219  \n",
      "1     2300 NW Thurman ST       NaN     OR         NaN  \n",
      "2  7510 N Charleston Ave  Portland     OR       97203  \n",
      "3    1525 SW Sunset Blvd  Portland     OR       97239  \n",
      "4       7860 SE 13th Ave  Portland     OR       97202  \n",
      "\n",
      "--- Final Data Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18 entries, 0 to 17\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   id              18 non-null     object\n",
      " 1   name            18 non-null     object\n",
      " 2   street_address  18 non-null     object\n",
      " 3   city            14 non-null     object\n",
      " 4   region          16 non-null     object\n",
      " 5   postal_code     15 non-null     object\n",
      "dtypes: object(6)\n",
      "memory usage: 996.0+ bytes\n",
      "\n",
      "Saved the final, clean libraries data to 'cleaned_libraries.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Load the libraries dataset ---\n",
    "libraries_df = pd.read_csv('data\\\\libraries.csv')\n",
    "\n",
    "# --- 2. Define Custom Function for Street Address ---\n",
    "def clean_street_address(address):\n",
    "    \"\"\"\n",
    "    Applies custom capitalization rules to a street address.\n",
    "    \"\"\"\n",
    "    if pd.isna(address):\n",
    "        return np.nan\n",
    "    \n",
    "    # Collapse multiple spaces and trim\n",
    "    address = str(address).strip()\n",
    "    address = ' '.join(address.split())\n",
    "    \n",
    "    # Process each word\n",
    "    processed_words = []\n",
    "    for word in address.split(' '):\n",
    "        if len(word) <= 2:\n",
    "            processed_words.append(word.upper()) # Uppercase for short words (SW, SE)\n",
    "        else:\n",
    "            processed_words.append(word.capitalize()) # Capitalize for others\n",
    "            \n",
    "    return ' '.join(processed_words)\n",
    "\n",
    "# Keep a copy of the original postal codes to count changes\n",
    "original_postal_codes = libraries_df['postal_code'].copy()\n",
    "\n",
    "# --- 3. Clean 'name' Column ---\n",
    "libraries_df['name'] = libraries_df['name'].astype(str).str.replace(r'\\s+', ' ', regex=True).str.strip().str.title()\n",
    "print(\"Cleaned 'name': Collapsed spaces and applied title case.\")\n",
    "\n",
    "# --- 4. Clean 'street_address' Column with Custom Function ---\n",
    "libraries_df['street_address'] = libraries_df['street_address'].apply(clean_street_address)\n",
    "print(\"Cleaned 'street_address': Applied custom capitalization.\")\n",
    "\n",
    "# --- 5. Clean 'city' Column ---\n",
    "libraries_df['city'] = libraries_df['city'].astype(str).str.strip().str.title()\n",
    "libraries_df['city'] = libraries_df['city'].replace('Nan', np.nan)\n",
    "print(\"Cleaned 'city': Trimmed whitespace and capitalized.\")\n",
    "\n",
    "# --- 6. Clean 'region' Column ---\n",
    "libraries_df['region'] = libraries_df['region'].astype(str).str.strip().str.upper()\n",
    "libraries_df['region'] = libraries_df['region'].replace('NAN', np.nan)\n",
    "print(\"Cleaned 'region': Trimmed whitespace and converted to uppercase.\")\n",
    "\n",
    "# --- 7. Clean 'postal_code' Column ---\n",
    "def clean_postal_code(code):\n",
    "    if pd.isna(code):\n",
    "        return np.nan\n",
    "    cleaned_code = ''.join(filter(str.isdigit, str(code)))\n",
    "    return cleaned_code if cleaned_code else np.nan\n",
    "\n",
    "libraries_df['postal_code'] = libraries_df['postal_code'].apply(clean_postal_code)\n",
    "changes = (libraries_df['postal_code'] != original_postal_codes).sum()\n",
    "print(f\"Cleaned 'postal_code': Removed special characters. Updated {changes} records.\")\n",
    "\n",
    "\n",
    "# --- 8. Final Output ---\n",
    "print(\"\\n--- Final Data Preview ---\")\n",
    "print(libraries_df.head())\n",
    "print(\"\\n--- Final Data Info ---\")\n",
    "libraries_df.info()\n",
    "\n",
    "# Save the cleaned data\n",
    "libraries_df.to_csv('cleaned_libraries.csv', index=False)\n",
    "print(\"\\nSaved the final, clean libraries data to 'cleaned_libraries.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92e4c094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced empty strings with null values in preparation for filling.\n",
      "Renamed columns for a clean merge.\n",
      "Merging data...\n",
      "Filled empty education, occupation, and gender fields with 'Unknown'.\n",
      "Validation 1: Removed 143 records where checkout occurred before the patron's birth date.\n",
      "Validation 2: Removed 53 records where checkout occurred before the book was published.\n",
      "\n",
      "Success! Your final master data file is ready for Power BI.\n",
      "Saved to 'master_library_data.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jovan\\AppData\\Local\\Temp\\ipykernel_2652\\2888261430.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  master_df[col].fillna(\"Unknown\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Handle Empty Strings in Customer Data ---\n",
    "# Replace empty strings \"\" with actual null values (NaN) first\n",
    "for col in ['education', 'occupation', 'gender']:\n",
    "    customers_df[col] = customers_df[col].replace(r'^\\s*$', np.nan, regex=True)\n",
    "print(\"Replaced empty strings with null values in preparation for filling.\")\n",
    "\n",
    "# --- 3. Rename Columns for a Clean Merge ---\n",
    "customers_df.rename(columns={'id': 'patron_id', 'name': 'customer_name', 'street_address': 'customer_address', 'city': 'customer_city'}, inplace=True)\n",
    "books_df.rename(columns={'id': 'book_id'}, inplace=True)\n",
    "libraries_df.rename(columns={'id': 'library_id', 'name': 'library_name', 'street_address': 'library_address', 'city': 'library_city'}, inplace=True)\n",
    "checkouts_df.rename(columns={'id': 'book_id'}, inplace=True)\n",
    "print(\"Renamed columns for a clean merge.\")\n",
    "\n",
    "# --- 4. Merge the DataFrames ---\n",
    "print(\"Merging data...\")\n",
    "master_df = pd.merge(checkouts_df, customers_df, on='patron_id', how='left')\n",
    "master_df = pd.merge(master_df, books_df, on='book_id', how='left')\n",
    "master_df = pd.merge(master_df, libraries_df, on='library_id', how='left')\n",
    "\n",
    "# --- 5. Fill Remaining Missing Values with \"Unknown\" ---\n",
    "# YOUR LOGIC: Fill NaN values in these specific columns\n",
    "for col in ['education', 'occupation', 'gender']:\n",
    "    master_df[col].fillna(\"Unknown\", inplace=True)\n",
    "print(\"Filled empty education, occupation, and gender fields with 'Unknown'.\")\n",
    "\n",
    "\n",
    "# --- 6. Final Cross-Table Validations ---\n",
    "master_df['birth_date'] = pd.to_datetime(master_df['birth_date'])\n",
    "master_df['date_checkout'] = pd.to_datetime(master_df['date_checkout'])\n",
    "\n",
    "# Validation 1: Patron's age\n",
    "initial_rows = len(master_df)\n",
    "master_df = master_df[master_df['date_checkout'] >= master_df['birth_date']].copy()\n",
    "print(f\"Validation 1: Removed {initial_rows - len(master_df)} records where checkout occurred before the patron's birth date.\")\n",
    "\n",
    "# Validation 2: Book's existence\n",
    "initial_rows = len(master_df)\n",
    "master_df = master_df[master_df['date_checkout'].dt.year >= master_df['publishedYear']].copy()\n",
    "print(f\"Validation 2: Removed {initial_rows - len(master_df)} records where checkout occurred before the book was published.\")\n",
    "\n",
    "# --- 7. Save the Final Master File ---\n",
    "master_df.to_csv('master_library_data.csv', index=False)\n",
    "print(\"\\nSuccess! Your final master data file is ready for Power BI.\")\n",
    "print(\"Saved to 'master_library_data.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
